{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LS_DS_222_assignment.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"6DuK8_BOdn4g"},"source":["Lambda School Data Science\n","\n","*Unit 2, Sprint 2, Module 2*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"7IXUfiQ2UKj6"},"source":["# Random Forests\n","\n","## Assignment\n","- [ ] Read [“Adopting a Hypothesis-Driven Workflow”](http://archive.is/Nu3EI), a blog post by a Lambda DS student about the Tanzania Waterpumps challenge.\n","- [ ] Continue to participate in our Kaggle challenge.\n","- [ ] Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features.\n","- [ ] Try Ordinal Encoding.\n","- [ ] Try a Random Forest Classifier.\n","- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n","- [ ] Commit your notebook to your fork of the GitHub repo.\n","\n","## Stretch Goals\n","\n","### Doing\n","- [ ] Add your own stretch goal(s) !\n","- [ ] Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n","- [ ] Try other [categorical encodings](https://contrib.scikit-learn.org/category_encoders/).\n","- [ ] Get and plot your feature importances.\n","- [ ] Make visualizations and share on Slack.\n","\n","### Reading\n","\n","Top recommendations in _**bold italic:**_\n","\n","#### Decision Trees\n","- A Visual Introduction to Machine Learning, [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/),  and _**[Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)**_\n","- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n","- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n","- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n","- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n","\n","#### Random Forests\n","- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 8: Tree-Based Methods\n","- [Coloring with Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)\n","- _**[Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)**_\n","\n","#### Categorical encoding for trees\n","- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n","- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n","- _**[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)**_\n","- _**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)**_\n","- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n","- [The Mechanics of Machine Learning, Chapter 6: Categorically Speaking](https://mlbook.explained.ai/catvars.html)\n","\n","#### Imposter Syndrome\n","- [Effort Shock and Reward Shock (How The Karate Kid Ruined The Modern World)](http://www.tempobook.com/2014/07/09/effort-shock-and-reward-shock/)\n","- [How to manage impostor syndrome in data science](https://towardsdatascience.com/how-to-manage-impostor-syndrome-in-data-science-ad814809f068)\n","- [\"I am not a real data scientist\"](https://brohrer.github.io/imposter_syndrome.html)\n","- _**[Imposter Syndrome in Data Science](https://caitlinhudon.com/2018/01/19/imposter-syndrome-in-data-science/)**_\n","\n","\n","### More Categorical Encodings\n","\n","**1.** The article **[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)** mentions 4 encodings:\n","\n","- **\"Categorical Encoding\":** This means using the raw categorical values as-is, not encoded. Scikit-learn doesn't support this, but some tree algorithm implementations do. For example, [Catboost](https://catboost.ai/), or R's [rpart](https://cran.r-project.org/web/packages/rpart/index.html) package.\n","- **Numeric Encoding:** Synonymous with Label Encoding, or \"Ordinal\" Encoding with random order. We can use [category_encoders.OrdinalEncoder](https://contrib.scikit-learn.org/category_encoders/ordinal.html).\n","- **One-Hot Encoding:** We can use [category_encoders.OneHotEncoder](https://contrib.scikit-learn.org/category_encoders/onehot.html).\n","- **Binary Encoding:** We can use [category_encoders.BinaryEncoder](https://contrib.scikit-learn.org/category_encoders/binary.html).\n","\n","\n","**2.** The short video \n","**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)** introduces an interesting idea: use both X _and_ y to encode categoricals.\n","\n","Category Encoders has multiple implementations of this general concept:\n","\n","- [CatBoost Encoder](https://contrib.scikit-learn.org/category_encoders/catboost.html)\n","- [Generalized Linear Mixed Model Encoder](https://contrib.scikit-learn.org/category_encoders/glmm.html)\n","- [James-Stein Encoder](https://contrib.scikit-learn.org/category_encoders/jamesstein.html)\n","- [Leave One Out](https://contrib.scikit-learn.org/category_encoders/leaveoneout.html)\n","- [M-estimate](https://contrib.scikit-learn.org/category_encoders/mestimate.html)\n","- [Target Encoder](https://contrib.scikit-learn.org/category_encoders/targetencoder.html)\n","- [Weight of Evidence](https://contrib.scikit-learn.org/category_encoders/woe.html)\n","\n","Category Encoder's mean encoding implementations work for regression problems or binary classification problems. \n","\n","For multi-class classification problems, you will need to temporarily reformulate it as binary classification. For example:\n","\n","```python\n","encoder = ce.TargetEncoder(min_samples_leaf=..., smoothing=...) # Both parameters > 1 to avoid overfitting\n","X_train_encoded = encoder.fit_transform(X_train, y_train=='functional')\n","X_val_encoded = encoder.transform(X_train, y_val=='functional')\n","```\n","\n","For this reason, mean encoding won't work well within pipelines for multi-class classification problems.\n","\n","**3.** The **[dirty_cat](https://dirty-cat.github.io/stable/)** library has a Target Encoder implementation that works with multi-class classification.\n","\n","```python\n"," dirty_cat.TargetEncoder(clf_type='multiclass-clf')\n","```\n","It also implements an interesting idea called [\"Similarity Encoder\" for dirty categories](https://www.slideshare.net/GaelVaroquaux/machine-learning-on-non-curated-data-154905090).\n","\n","However, it seems like dirty_cat doesn't handle missing values or unknown categories as well as category_encoders does. And you may need to use it with one column at a time, instead of with your whole dataframe.\n","\n","**4. [Embeddings](https://www.kaggle.com/colinmorris/embedding-layers)** can work well with sparse / high cardinality categoricals.\n","\n","_**I hope it’s not too frustrating or confusing that there’s not one “canonical” way to encode categoricals. It’s an active area of research and experimentation — maybe you can make your own contributions!**_"]},{"cell_type":"markdown","metadata":{"id":"TN6EEoyedn4l"},"source":["### Setup\n","\n","You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."]},{"cell_type":"code","metadata":{"id":"o9eSnDYhUGD7","executionInfo":{"status":"ok","timestamp":1605504149910,"user_tz":420,"elapsed":3357,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}}},"source":["%%capture\n","import sys\n","\n","# If you're on Colab:\n","if 'google.colab' in sys.modules:\n","    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n","    !pip install category_encoders==2.*\n","\n","# If you're working locally:\n","else:\n","    DATA_PATH = '../data/'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJBD4ruICm1m","executionInfo":{"status":"ok","timestamp":1605504157604,"user_tz":420,"elapsed":4427,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}},"outputId":"f9ca6cd1-d2a6-49bd-dd5b-4bb6a8cc9448","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n","                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n","test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n","sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n","\n","train.shape, test.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((59400, 41), (14358, 40))"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"qWxr78-sdn4v","executionInfo":{"status":"ok","timestamp":1605504165098,"user_tz":420,"elapsed":4385,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import datetime\n","\n","# Retrieve data\n","train_val = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n","                     pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n","test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n","\n","target = 'status_group'\n","\n","# Wrangle train, validate, and test sets in the same way\n","def wrangle(X):\n","    \n","    # Prevent SettingWithCopyWarning\n","    X = X.copy()\n","\n","    if target in X.columns:\n","      X = X.drop(columns=[target])\n","    \n","    # About 3% of the time, latitude has small values near zero,\n","    # outside Tanzania, so we'll treat these values like zero.\n","    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n","    \n","    # When columns have zeros and shouldn't, they are like null values.\n","    # So we will replace the zeros with nulls, and impute missing values later.\n","    # Also create a \"missing indicator\" column, because the fact that\n","    # values are missing may be a predictive signal.\n","    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n","                       'gps_height', 'population']\n","    for col in cols_with_zeros:\n","        X[col] = X[col].replace(0, np.nan)\n","        X[col+'_MISSING'] = X[col].isnull()\n","            \n","    # Drop duplicate columns\n","    duplicates = ['quantity_group', 'payment_type']\n","    X = X.drop(columns=duplicates)\n","    \n","    # Drop recorded_by (never varies) and id (always varies, random)\n","    unusable_variance = ['recorded_by', 'id']\n","    X = X.drop(columns=unusable_variance)\n","    \n","    # Convert date_recorded to datetime\n","    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n","    \n","    # Extract components from date_recorded, then drop the original column\n","    X['year_recorded'] = X['date_recorded'].dt.year\n","    X['month_recorded'] = X['date_recorded'].dt.month\n","    X['day_recorded'] = X['date_recorded'].dt.day\n","    X = X.drop(columns='date_recorded')\n","    \n","    # Engineer feature: how many years from construction_year to date_recorded\n","    X['years'] = X['year_recorded'] - X['construction_year']\n","    X['years_MISSING'] = X['years'].isnull()\n","  \n","    return X\n","\n","# Split train & val\n","# train, val = train_test_split(\n","#     train_val,\n","#     train_size = 0.80,\n","#     test_size = 0.20,\n","#     stratify = train_val[target],\n","#     random_state = train_val_seed,\n","#     )\n","\n","# Actually wrangle and split the data up\n","X_train_val, y_train_val = wrangle(train_val), train_val[target]\n","X_test                   = wrangle(test)  # y_test is not provided\n","\n","# Actually wrangle and split the data up\n","# X_train, y_train = wrangle(train), train[target]\n","# X_val, y_val     = wrangle(val), val[target]\n","# X_test           = wrangle(test)  # y_test is not provided\n","\n","# print('Shapes:')\n","# print('Train, X_train, y_train:', train.shape, X_train.shape, y_train.shape)\n","# print('Val, X_val, y_val:', val.shape, X_val.shape, y_val.shape)\n","# print('Test, X_test:', test.shape, X_test.shape)\n","\n","from category_encoders import OrdinalEncoder\n","from category_encoders import TargetEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Make a pipeline for encoding\n","encoder = Pipeline(steps=[\n","    ('ordinalencoder', OrdinalEncoder(return_df=True, handle_unknown='return_nan', handle_missing='return_nan')),\n","    # ('targetencoder', TargetEncoder(min_samples_leaf=1, smoothing=1)), \n","    ('imputer', SimpleImputer()),\n","    ('scaler', StandardScaler()),\n","    ])\n","X_train_val_encoded = encoder.fit_transform(X_train_val)\n","# X_val_encoded = encoder.transform(X_val)\n","X_test_encoded = encoder.transform(X_test)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfpREt08eBaO","executionInfo":{"status":"ok","timestamp":1605507553739,"user_tz":420,"elapsed":646,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}}},"source":["def try_model(seed=42, max_depth=None, n_estimators=20, split_samples=2, leaf_samples=1):\n","\n","    # Split train & val\n","    X_train_encoded, X_val_encoded, y_train, y_val = train_test_split(\n","        X_train_val_encoded,\n","        y_train_val,\n","        train_size = 0.80,\n","        test_size = 0.20,\n","        stratify = train_val[target],\n","        random_state = seed,\n","        )\n","\n","    # Make and fit the model\n","    model = Pipeline(steps=[\n","        ('randomforestclassifier', RandomForestClassifier(\n","            max_depth = max_depth,\n","            n_estimators = n_estimators,\n","            min_samples_split = split_samples, \n","            min_samples_leaf = leaf_samples,\n","            random_state = seed,\n","            ), \n","        ),\n","        ])\n","    model.fit(X_train_encoded, y_train)\n","\n","    # Evaluate the model\n","    train_score = model.score(X_train_encoded, y_train)\n","    val_score = model.score(X_val_encoded, y_val)\n","    # print('Train Accuracy: %.3f' % train_score)\n","    # print('Validation Accuracy: %.3f' % val_score)\n","\n","    # Combine these two into a \"Cautious Accuracy\"\n","    # Take the difference between the train and val scores and subtract that from val_score\n","    # Rewards a high val score, but only if the model is not overfit.\n","    cautious_score = 2 * val_score - train_score\n","    # print('Cautious Accuracy: %.3f' % cautious_score)\n","    return cautious_score"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVw_yT6KeC5m","executionInfo":{"status":"ok","timestamp":1605505214716,"user_tz":420,"elapsed":427870,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}},"outputId":"8a573d65-ea4a-45be-8967-dd5211822676","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Do a grid search over random forests\n","for depth in range(10,71,20):\n","  for n_estimators in range(18,23,2):\n","    for min_samples in range(10,31,10):\n","      scores = [try_model(seed=seed, max_depth=depth, n_estimators=n_estimators, min_samples=min_samples) for seed in range(5)]\n","      mean_score = np.array(scores).mean()\n","      print('Depth =',depth,'Trees =',n_estimators,'Min Samples/Split =',min_samples,'Mean Score =',mean_score)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Depth = 10 Trees = 18 Min Samples/Split = 10 Mean Score = 0.7401304713804713\n","Depth = 10 Trees = 18 Min Samples/Split = 20 Mean Score = 0.741797138047138\n","Depth = 10 Trees = 18 Min Samples/Split = 30 Mean Score = 0.7422937710437711\n","Depth = 10 Trees = 20 Min Samples/Split = 10 Mean Score = 0.7394949494949494\n","Depth = 10 Trees = 20 Min Samples/Split = 20 Mean Score = 0.7432070707070709\n","Depth = 10 Trees = 20 Min Samples/Split = 30 Mean Score = 0.7415656565656565\n","Depth = 10 Trees = 22 Min Samples/Split = 10 Mean Score = 0.7404040404040404\n","Depth = 10 Trees = 22 Min Samples/Split = 20 Mean Score = 0.7429671717171716\n","Depth = 10 Trees = 22 Min Samples/Split = 30 Mean Score = 0.7464814814814814\n","Depth = 30 Trees = 18 Min Samples/Split = 10 Mean Score = 0.6978072390572391\n","Depth = 30 Trees = 18 Min Samples/Split = 20 Mean Score = 0.731712962962963\n","Depth = 30 Trees = 18 Min Samples/Split = 30 Mean Score = 0.742087542087542\n","Depth = 30 Trees = 20 Min Samples/Split = 10 Mean Score = 0.7017760942760942\n","Depth = 30 Trees = 20 Min Samples/Split = 20 Mean Score = 0.7301010101010101\n","Depth = 30 Trees = 20 Min Samples/Split = 30 Mean Score = 0.7423569023569024\n","Depth = 30 Trees = 22 Min Samples/Split = 10 Mean Score = 0.7010521885521885\n","Depth = 30 Trees = 22 Min Samples/Split = 20 Mean Score = 0.7342171717171716\n","Depth = 30 Trees = 22 Min Samples/Split = 30 Mean Score = 0.742003367003367\n","Depth = 50 Trees = 18 Min Samples/Split = 10 Mean Score = 0.6978156565656566\n","Depth = 50 Trees = 18 Min Samples/Split = 20 Mean Score = 0.7310185185185185\n","Depth = 50 Trees = 18 Min Samples/Split = 30 Mean Score = 0.7409217171717171\n","Depth = 50 Trees = 20 Min Samples/Split = 10 Mean Score = 0.6968308080808081\n","Depth = 50 Trees = 20 Min Samples/Split = 20 Mean Score = 0.7326304713804715\n","Depth = 50 Trees = 20 Min Samples/Split = 30 Mean Score = 0.7414520202020203\n","Depth = 50 Trees = 22 Min Samples/Split = 10 Mean Score = 0.6961784511784512\n","Depth = 50 Trees = 22 Min Samples/Split = 20 Mean Score = 0.7326430976430977\n","Depth = 50 Trees = 22 Min Samples/Split = 30 Mean Score = 0.7419949494949496\n","Depth = 70 Trees = 18 Min Samples/Split = 10 Mean Score = 0.6976936026936027\n","Depth = 70 Trees = 18 Min Samples/Split = 20 Mean Score = 0.7284301346801347\n","Depth = 70 Trees = 18 Min Samples/Split = 30 Mean Score = 0.7401052188552187\n","Depth = 70 Trees = 20 Min Samples/Split = 10 Mean Score = 0.6980808080808081\n","Depth = 70 Trees = 20 Min Samples/Split = 20 Mean Score = 0.7302819865319866\n","Depth = 70 Trees = 20 Min Samples/Split = 30 Mean Score = 0.7411700336700336\n","Depth = 70 Trees = 22 Min Samples/Split = 10 Mean Score = 0.7003282828282827\n","Depth = 70 Trees = 22 Min Samples/Split = 20 Mean Score = 0.730395622895623\n","Depth = 70 Trees = 22 Min Samples/Split = 30 Mean Score = 0.7457954545454546\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nnFpWBPWj3Rr","executionInfo":{"status":"ok","timestamp":1605507086058,"user_tz":420,"elapsed":146180,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}},"outputId":"0a3c6708-bf44-4091-9232-80ca512aaa18","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Do a grid search over random forests\n","for n_estimators in range(20,21):\n","  for min_samples in range(25,76,10):\n","    scores = [try_model(seed=seed, max_depth=None, n_estimators=n_estimators, min_samples=min_samples) for seed in range(10)]\n","    scores = np.array(scores)\n","    mean_score = scores.mean()\n","    std_score = scores.std()\n","    low_score = format(mean_score - std_score, '.3f')\n","    high_score = format(mean_score + std_score, '.3f')\n","    print('Trees =',n_estimators,'Min Samples/Split =',min_samples,'Scores =',low_score,'-',high_score)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Depth = full, Trees = 20 Min Samples/Split = 25 Scores = 0.731 - 0.744\n","Depth = full, Trees = 20 Min Samples/Split = 35 Scores = 0.742 - 0.754\n","Depth = full, Trees = 20 Min Samples/Split = 45 Scores = 0.746 - 0.761\n","Depth = full, Trees = 20 Min Samples/Split = 55 Scores = 0.749 - 0.764\n","Depth = full, Trees = 20 Min Samples/Split = 65 Scores = 0.750 - 0.764\n","Depth = full, Trees = 20 Min Samples/Split = 75 Scores = 0.753 - 0.767\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mEwW2W0_svhT","executionInfo":{"status":"ok","timestamp":1605511031547,"user_tz":420,"elapsed":394261,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}},"outputId":"6b73309a-33e6-4e96-ffeb-57fdf0cc26b9","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Do a grid search over random forests\n","for split_samples in range(52,57,2):\n","  print()\n","  for leaf_samples in range(10,21,5):\n","    scores = [try_model(seed=seed, split_samples=split_samples, leaf_samples=leaf_samples) for seed in range(20)]\n","    scores = np.array(scores)\n","    mean_score = scores.mean()\n","    std_score = scores.std()\n","    low_score = format(mean_score - std_score, '.4f')\n","    high_score = format(mean_score + std_score, '.4f')\n","    print(f'Min Samples/Split = {split_samples}, Min Samples/Leaf = {leaf_samples}, Scores = {low_score} - {high_score}')"],"execution_count":44,"outputs":[{"output_type":"stream","text":["\n","Min Samples/Split = 52, Min Samples/Leaf = 10, Scores = 0.751 - 0.768\n","Min Samples/Split = 52, Min Samples/Leaf = 15, Scores = 0.753 - 0.767\n","Min Samples/Split = 52, Min Samples/Leaf = 20, Scores = 0.753 - 0.767\n","\n","Min Samples/Split = 54, Min Samples/Leaf = 10, Scores = 0.751 - 0.766\n","Min Samples/Split = 54, Min Samples/Leaf = 15, Scores = 0.753 - 0.764\n","Min Samples/Split = 54, Min Samples/Leaf = 20, Scores = 0.753 - 0.766\n","\n","Min Samples/Split = 56, Min Samples/Leaf = 10, Scores = 0.754 - 0.766\n","Min Samples/Split = 56, Min Samples/Leaf = 15, Scores = 0.751 - 0.766\n","Min Samples/Split = 56, Min Samples/Leaf = 20, Scores = 0.753 - 0.765\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HAmrzHVXP9Gb"},"source":["\n","from sklearn.model_selection import cross_val_score\n","k = 3\n","scores = cross_val_score(pipeline, X_train, y_train, cv=k, \n","                         scoring='neg_mean_absolute_error')\n","print(f'MAE for {k} folds:', -scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zYzT1WkhQhL","executionInfo":{"status":"ok","timestamp":1605511501807,"user_tz":420,"elapsed":4967,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}},"outputId":"186d5118-cd87-44b6-f17d-70e6bfbb0308","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Make a model with a certain hyperparameter combo\n","# train_val_seed=42\n","max_depth=None\n","n_estimators=40\n","split_samples=52\n","leaf_samples=15\n","\n","# Split train & val\n","X_train_encoded, X_val_encoded, y_train, y_val = train_test_split(\n","    X_train_val_encoded,\n","    y_train_val,\n","    train_size = 0.80,\n","    test_size = 0.20,\n","    stratify = train_val[target],\n","    # random_state = train_val_seed,\n","    )\n","\n","# Make and fit the model\n","model = Pipeline(steps=[\n","    ('randomforestclassifier', RandomForestClassifier(\n","          max_depth = max_depth,\n","          n_estimators = n_estimators,\n","          min_samples_split = split_samples, \n","          min_samples_leaf = leaf_samples,\n","        ), \n","    ),\n","    ])\n","model.fit(X_train_encoded, y_train)\n","\n","# Evaluate the model\n","train_score = model.score(X_train_encoded, y_train)\n","val_score = model.score(X_val_encoded, y_val)\n","print('Train Accuracy: %.3f' % train_score)\n","print('Validation Accuracy: %.3f' % val_score)\n","\n","# Combine these two into a \"Cautious Accuracy\"\n","# Take the difference between the train and val scores and subtract that from val_score\n","# Rewards a high val score, but only if the model is not overfit.\n","cautious_score = 2 * val_score - train_score\n","print('Cautious Accuracy: %.3f' % cautious_score)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Train Accuracy: 0.811\n","Validation Accuracy: 0.791\n","Cautious Accuracy: 0.772\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kZWLlDKLekWl","executionInfo":{"status":"ok","timestamp":1605511512295,"user_tz":420,"elapsed":706,"user":{"displayName":"James Slagle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHRAEz2_G1d3HzFY6upFC6aFaltoE1eE_4Xy_lQQ=s64","userId":"05173057312343357198"}},"outputId":"955b8cb7-4452-4ef5-bb77-9f20eca6e1ae","colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["# Create the test prediction and submission file\n","from google.colab import files\n","\n","y_pred = model.predict(X_test_encoded)\n","message = 'Something something'\n","submission = pd.DataFrame({'id': test.id, 'status_group': y_pred})\n","submission_filename = 'submission.csv'\n","submission.to_csv(submission_filename, index=False)\n","files.download(submission_filename)\n","# !kaggle competitions submit -c dspt-pump-it-up-challenge -f submission.csv -m \"{message}\""],"execution_count":50,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_e10bf981-f53a-42c4-ae69-4a57285044ea\", \"submission.csv\", 264166)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}